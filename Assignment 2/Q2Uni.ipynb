{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_RNN Akhilesh Ravi 16110007_Unigram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbZfUNRMWRM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import random\n",
        "import collections\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43fSYzvKpuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from operator import itemgetter\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from string import punctuation\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o4OB5GTMGjr",
        "colab_type": "code",
        "outputId": "6c596e05-4ba1-4d92-d26a-e067f530902c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWk36_zK7bI",
        "colab_type": "code",
        "outputId": "1013ab11-2bd7-4f30-9c13-064d231ee324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Pre-Processing Dataset -Filtering out symbols, newlines. \n",
        "#Parsing and dividing in 80-20 ratio\n",
        "\n",
        "\n",
        "# DATASET- Jane Austen Novels: The Complete Works of Jane Austen \n",
        "#take one line, neglect empty line and then add it to line_set then lower down whole set(string)\n",
        "with open(\"JaneAusten.txt\", 'rt') as source_file:  \n",
        "    line_set = ['']\n",
        "    for line in (l.rstrip() for l in source_file):\n",
        "        if line != '' or line_set[-1] != '\\n':                 \n",
        "            line_set.append(line + '\\n')\n",
        "    text = \"\".join(line_set)\n",
        "text = text.lower()  \n",
        "\n",
        "# Tokenizing sentences from processed text\n",
        "vocabulary = {}#initilaizing vocab dict\n",
        "corpus = []#intializing corpus\n",
        "sentence_set = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentence_set: #taking one sentence from sentence list\n",
        "    words = word_tokenize(sentence) #work tokenizing each word of that sentence\n",
        "    processed_sentence = ['<s>'] #making processed sentence out of it and adding START <s> symbol \n",
        "    for word in words:\n",
        "        if len(set(word).intersection(punctuation)) == 0 and (len(word)>1 or word == 'a' or word == 'i') :\n",
        "            processed_sentence.append(word)#adding processed and selected word to my new processed sentence\n",
        "            if word in vocabulary.keys():#adding in vobaulary list if not exist else increasing frequecny by 1 if exists\n",
        "                vocabulary[word] += 1\n",
        "            else:\n",
        "                vocabulary[word] = 1\n",
        "                \n",
        "    processed_sentence.append('</s>')#adding END </s> to the end of my sentence\n",
        "    corpus.append(\" \".join(processed_sentence))#adding processed sentence to the corpus\n",
        "\n",
        "#Now Processed Corpus is ready    \n",
        "number_of_sentence = len(corpus)\n",
        "# Vocabulary list doesn't contain the START and END word used in Pre-Processing\n",
        "vocabulary['<s>'] = 2*number_of_sentence\n",
        "vocabulary['</s>'] = 2*number_of_sentence\n",
        "# Now we have Processed sentence corpus and vocabulary list. \n",
        "#Processed Corpus DATA ANALYSIS  \n",
        "tokens = sum(vocabulary.values())\n",
        "types = len(vocabulary)\n",
        "print ('Number of Sentences = ' + str(number_of_sentence))\n",
        "print ('Number of Tokens = ' + str(tokens))\n",
        "print ('Number of Types = ' + str(types))\n",
        "print (\"TTR= \" + str(types/tokens))\n",
        "# train-test split in 80:20 ratio\n",
        "train_data, test_data = train_test_split(corpus, test_size=0.20, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences = 35056\n",
            "Number of Tokens = 914604\n",
            "Number of Types = 14651\n",
            "TTR= 0.01601895465141198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg-sAza9MwmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # tf.reset_default_graph()\n",
        "\n",
        "start_time = time.time()\n",
        "def elapsed(sec):\n",
        "    if sec<60:\n",
        "        return str(sec) + \" sec\"\n",
        "    elif sec<(60*60):\n",
        "        return str(sec/60) + \" min\"\n",
        "    else:\n",
        "        return str(sec/(60*60)) + \" hr\"\n",
        "\n",
        "def read_data(content):\n",
        "    content = [x.strip() for x in content]\n",
        "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
        "    content = np.array(content)\n",
        "    return content\n",
        "\n",
        "# new_train = []\n",
        "# for i in train_text:\n",
        "#   new = ''.join(i.split('<UNK>'))\n",
        "#   new_train.append(new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7vK1JIJkhV7",
        "colab_type": "code",
        "outputId": "95b05891-580f-435a-a0aa-876f0bf2950a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "training_data = read_data(train_data)\n",
        "testing_data = read_data(test_data)\n",
        "print(\"Loaded training data...\")\n",
        "\n",
        "def build_dataset(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary\n",
        "\n",
        "dictionary, reverse_dictionary = build_dataset(training_data)\n",
        "vocab_size = len(dictionary)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded training data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p28wtguNKnNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = dictionary.copy()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "# word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 1  # unknown\n",
        "word_index[\"<UNUSED>\"] = 2\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "train_text = []\n",
        "for i in train_data:\n",
        "  train_text.append(decode_review(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvEERT7kVgz2",
        "colab": {}
      },
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ow3OBsjWcdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkdQu05-d78c",
        "colab_type": "code",
        "outputId": "20ea6605-ed54-4ee1-a38b-0bd720dd1cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "# training_iters = 10 * len(training_data) #10000\n",
        "training_iters = 20000\n",
        "display_step = 1000\n",
        "n_input = 4\n",
        "\n",
        "\n",
        "logs_path = '/tmp/tensorflow/rnn_words'\n",
        "writer = tf.summary.FileWriter(logs_path)\n",
        "\n",
        "# number of units in RNN cell\n",
        "n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    # reshape to [1, n_input]\n",
        "    x = tf.reshape(x, [-1, n_input])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
        "    x = tf.split(x,n_input,1)\n",
        "\n",
        "    # 2-layer LSTM, each layer has n_hidden units.\n",
        "    #rnn_cell = rnn.MultiRNNCell([rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE),rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)])\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    #rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "    \n",
        "    # 1-layer RNN with n_hidden units \n",
        "    #rnn_cell = rnn.MultiRNNCell([rnn.GRUCell(n_hidden), rnn.GRUCell(n_hidden)])\n",
        "    rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
        "    # rnns.append(rnn.BasicRNNCell(n_hidden))\n",
        "\n",
        "    # generate prediction\n",
        "    # outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # there are n_input outputs but\n",
        "    # we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_input+1)\n",
        "    end_offset = n_input + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_input+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"No.= \" + str((step+1)//display_step) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
        "            symbols_out = training_data[offset + n_input]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
        "        step += 1\n",
        "        offset += (n_input+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "\n",
        "\n",
        "    num_start = 0\n",
        "    n_input = 4\n",
        "    words = [word_index['<PAD>']] * (n_input-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < n_input and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-2f3451b4c47a>:43: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-10-2f3451b4c47a>:48: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f7efe249da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-10-2f3451b4c47a>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "No.= 1, Average Loss= 30.414194, Average Accuracy= 0.40%\n",
            "['fear', 'that', 'you', 'will'] - [be] vs [imposition]\n",
            "No.= 2, Average Loss= 24.301142, Average Accuracy= 0.20%\n",
            "['forgive', 'he', 'is', 'a'] - [fortunate] vs [a]\n",
            "No.= 3, Average Loss= 18.366001, Average Accuracy= 0.40%\n",
            "['me', 'in', 'the', 'rank'] - [of] vs [be]\n",
            "No.= 4, Average Loss= 14.097642, Average Accuracy= 3.80%\n",
            "['</s>', '<s>', 'there', 'were'] - [but] vs [i]\n",
            "No.= 5, Average Loss= 10.865992, Average Accuracy= 7.70%\n",
            "['natural', 'as', 'there', 'was'] - [a] vs [the]\n",
            "No.= 6, Average Loss= 9.573772, Average Accuracy= 6.90%\n",
            "['did', 'not', 'prevent', 'my'] - [taking] vs [and]\n",
            "No.= 7, Average Loss= 8.498811, Average Accuracy= 7.00%\n",
            "['she', 'tell', 'her', 'sister'] - [</s>] vs [</s>]\n",
            "No.= 8, Average Loss= 7.875262, Average Accuracy= 10.20%\n",
            "['feel', 'none', '</s>', '<s>'] - [what] vs [i]\n",
            "No.= 9, Average Loss= 8.165482, Average Accuracy= 9.10%\n",
            "['in', 'the', 'neighbourhood', 'it'] - [was] vs [the]\n",
            "No.= 10, Average Loss= 7.720201, Average Accuracy= 7.80%\n",
            "['</s>', '<s>', 'we', 'were'] - [speaking] vs [</s>]\n",
            "No.= 11, Average Loss= 7.786657, Average Accuracy= 9.10%\n",
            "['it', 'was', 'enough', 'was'] - [not] vs [the]\n",
            "No.= 12, Average Loss= 7.927349, Average Accuracy= 6.90%\n",
            "['for', 'the', 'ball', 'so'] - [near] vs [a]\n",
            "No.= 13, Average Loss= 7.910229, Average Accuracy= 6.70%\n",
            "['ear', 'expressed', 'ah', '</s>'] - [<s>] vs [<s>]\n",
            "No.= 14, Average Loss= 7.883097, Average Accuracy= 8.60%\n",
            "['to', 'pursue', 'their', 'first'] - [plan] vs [</s>]\n",
            "No.= 15, Average Loss= 7.934909, Average Accuracy= 7.20%\n",
            "['the', 'kindness', 'of', 'henry'] - [in] vs [to]\n",
            "No.= 16, Average Loss= 7.849763, Average Accuracy= 6.30%\n",
            "['side', 'and', 'explanations', 'on'] - [miss] vs [the]\n",
            "No.= 17, Average Loss= 7.899747, Average Accuracy= 7.50%\n",
            "['spirits', 'began', 'to', 'revive'] - [and] vs [in]\n",
            "No.= 18, Average Loss= 7.802235, Average Accuracy= 7.50%\n",
            "['this', 'day', 'of', 'pleasure'] - [</s>] vs [of]\n",
            "No.= 19, Average Loss= 7.672790, Average Accuracy= 8.30%\n",
            "['a', 'good', 'excuse', 'and'] - [he] vs [<s>]\n",
            "No.= 20, Average Loss= 7.735665, Average Accuracy= 8.00%\n",
            "['manner', 'that', 'must', 'be'] - [highly] vs [the]\n",
            "Optimization Finished!\n",
            "Elapsed time:  2.428032954533895 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnToOoNPxvGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_perplexity = tf.exp(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ERkh3Fiur0E",
        "colab_type": "code",
        "outputId": "926f9f4d-5833-4664-ce1d-eaaf89c97ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as session:\n",
        "  print(session.run(train_perplexity))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "110084.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTsd0-JYVvr6",
        "colab_type": "code",
        "outputId": "c1749953-f0ee-493f-e830-4a5e37093093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "source": [
        "# Parameters\n",
        "# learning_rate = 0.0001\n",
        "# # training_iters = 10 * len(training_data) #10000\n",
        "# training_iters = 100\n",
        "# display_step = 10\n",
        "# n_input = 4\n",
        "\n",
        "\n",
        "\n",
        "# number of units in RNN cell\n",
        "# n_hidden = 128\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
        "y = tf.placeholder(\"float\", [None, vocab_size])\n",
        "\n",
        "# RNN output node weights and biases\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
        "}\n",
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    # reshape to [1, n_input]\n",
        "    x = tf.reshape(x, [-1, n_input])\n",
        "\n",
        "    # Generate a n_input-element sequence of inputs\n",
        "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
        "    x = tf.split(x,n_input,1)\n",
        "\n",
        "    # 1-layer LSTM with n_hidden units \n",
        "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
        "\n",
        "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # there are n_input outputs but\n",
        "    # we only want the last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Model evaluation\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as session:\n",
        "    session.run(init)\n",
        "    step = 0\n",
        "    offset = random.randint(0,n_input+1)\n",
        "    end_offset = n_input + 1\n",
        "    acc_total = 0\n",
        "    loss_total = 0\n",
        "    loss_overall = 0\n",
        "    writer.add_graph(session.graph)\n",
        "\n",
        "    while step < training_iters:\n",
        "        # Generate a minibatch. Add some randomness on selection process.\n",
        "        if offset > (len(training_data)-end_offset):\n",
        "            offset = random.randint(0, n_input+1)\n",
        "\n",
        "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
        "        symbols_in_keys = list(np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]))\n",
        "\n",
        "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
        "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
        "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
        "\n",
        "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
        "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
        "        loss_total += loss\n",
        "        loss_overall += loss\n",
        "        acc_total += acc\n",
        "        if (step+1) % display_step == 0:\n",
        "            print(\"No.= \" + str((step+1)//display_step) + \", Average Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
        "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
        "            acc_total = 0\n",
        "            loss_total = 0\n",
        "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
        "            symbols_out = training_data[offset + n_input]\n",
        "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
        "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
        "        step += 1\n",
        "        offset += (n_input+1)\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
        "    \n",
        "\n",
        "\n",
        "    num_start = 0\n",
        "    # n_input = 4\n",
        "    words = [word_index['<PAD>']] * (n_input-1) + ['<s>']\n",
        "    overall = list(words)\n",
        "    sentence = ''\n",
        "    num_words = 0\n",
        "    while num_start < 5 and num_words < 500:\n",
        "        try:\n",
        "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
        "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
        "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
        "            if onehot_pred_index == dictionary['</s>']:\n",
        "                sentence += \". \"\n",
        "                num_start += 1\n",
        "            else:\n",
        "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        except:\n",
        "            onehot_pred_index = dictionary['the']\n",
        "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
        "            symbols_in_keys = list(symbols_in_keys[1:])\n",
        "            symbols_in_keys.append(onehot_pred_index)\n",
        "        num_words += 1\n",
        "        \n",
        "\n",
        "    print(sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-fa5115eb013d>:22: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7efdf5de10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "No.= 1, Average Loss= 14.133432, Average Accuracy= 0.00%\n",
            "['some', 'reason', 'to', 'fear'] - [that] vs [costs]\n",
            "No.= 2, Average Loss= 10.907668, Average Accuracy= 1.70%\n",
            "['all', 'delighted', 'to', 'forgive'] - [he] vs [excursions]\n",
            "No.= 3, Average Loss= 9.728909, Average Accuracy= 5.40%\n",
            "['enough', 'to', 'support', 'me'] - [in] vs [the]\n",
            "No.= 4, Average Loss= 9.164069, Average Accuracy= 6.40%\n",
            "['rudely', 'and', 'coarse', '</s>'] - [<s>] vs [<s>]\n",
            "No.= 5, Average Loss= 8.671674, Average Accuracy= 7.10%\n",
            "['which', 'appeared', 'perfectly', 'natural'] - [as] vs [</s>]\n",
            "No.= 6, Average Loss= 8.217985, Average Accuracy= 8.90%\n",
            "['feelings', 'you', 'see', 'did'] - [not] vs [the]\n",
            "No.= 7, Average Loss= 8.225172, Average Accuracy= 9.40%\n",
            "['to', 'hope', 'that', 'she'] - [tell] vs [a]\n",
            "No.= 8, Average Loss= 7.744772, Average Accuracy= 8.50%\n",
            "['she', 'ought', 'to', 'feel'] - [none] vs [of]\n",
            "No.= 9, Average Loss= 7.817032, Average Accuracy= 7.80%\n",
            "['a', 'militia', 'regiment', 'in'] - [the] vs [the]\n",
            "No.= 10, Average Loss= 7.770150, Average Accuracy= 6.90%\n",
            "['elizabeth', 'was', 'distressed', '</s>'] - [<s>] vs [<s>]\n",
            "No.= 11, Average Loss= 7.496037, Average Accuracy= 8.40%\n",
            "['as', 'she', 'entered', 'it'] - [was] vs [her]\n",
            "No.= 12, Average Loss= 7.520935, Average Accuracy= 8.40%\n",
            "['</s>', '<s>', 'as', 'for'] - [the] vs [a]\n",
            "No.= 13, Average Loss= 7.286066, Average Accuracy= 8.10%\n",
            "['ejaculation', 'in', 'emma', 'ear'] - [expressed] vs [</s>]\n",
            "No.= 14, Average Loss= 7.368999, Average Accuracy= 8.70%\n",
            "['in', 'town', 'than', 'to'] - [pursue] vs [<s>]\n",
            "No.= 15, Average Loss= 7.286910, Average Accuracy= 9.20%\n",
            "['london', 'and', 'of', 'the'] - [kindness] vs [<s>]\n",
            "No.= 16, Average Loss= 7.416492, Average Accuracy= 7.90%\n",
            "['congratulations', 'on', 'her', 'side'] - [and] vs [of]\n",
            "No.= 17, Average Loss= 7.196128, Average Accuracy= 8.40%\n",
            "['for', 'when', 'her', 'spirits'] - [began] vs [of]\n",
            "No.= 18, Average Loss= 7.236237, Average Accuracy= 10.30%\n",
            "['questionable', 'enjoyments', 'of', 'this'] - [day] vs [the]\n",
            "No.= 19, Average Loss= 7.141064, Average Accuracy= 9.70%\n",
            "['that', 'would', 'be', 'a'] - [good] vs [be]\n",
            "No.= 20, Average Loss= 7.194935, Average Accuracy= 9.30%\n",
            "['openness', 'in', 'his', 'manner'] - [that] vs [of]\n",
            "Optimization Finished!\n",
            "Elapsed time:  7.298204986254374 min\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_dZ0I9jXJUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_perplexity = tf.exp(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFSvXZ4EXM3u",
        "colab_type": "code",
        "outputId": "af536a12-893a-4c01-8e13-367a2deaf842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with tf.Session() as session:\n",
        "  print(session.run(train_perplexity))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21526436.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}