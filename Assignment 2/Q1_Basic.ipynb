{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment_2: Akhilesh Ravi-16110011\n",
    "#Using Jane Austen Novels for Language Model Generation\n",
    "\n",
    "# Importing all packages\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET- Jane Austen Novels: The Complete Works of Jane Austen take one line, neglect empty line and then add it to line_set then lower down whole set(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences = 35056\n",
      "Number of Types = 14651\n",
      "Number of Tokens = 914639\n",
      "TTR= 0.01601834166266691\n"
     ]
    }
   ],
   "source": [
    "with open(\"JaneAusten.txt\", 'r') as file:  \n",
    "    lines = ['']\n",
    "    for line in (l.rstrip() for l in file):\n",
    "        if line != '' or lines[-1] != '\\n':                 \n",
    "            lines.append(line + '\\n')\n",
    "    text = \"\".join(lines)\n",
    "text = text.lower()  \n",
    "\n",
    "# Tokenizing sentences\n",
    "# Vocabulary dictionary and corpus\n",
    "vocabulary = {}\n",
    "corpus = []\n",
    "list_sentence = sent_tokenize(text)\n",
    "# <s> is added to the start of each sentence\n",
    "# </s> is added to the end of sentences\n",
    "for sentence in list_sentence:\n",
    "    word_list = word_tokenize(sentence) # tokenizing each word of that sentence\n",
    "    final_sentence = ['<s>']\n",
    "    for word in word_list:\n",
    "        # Obtains the frequency of the words in the corpus\n",
    "        if len(set(word).intersection(punctuation)) == 0 and (len(word)>1 or word == 'a' or word == 'i') :\n",
    "            final_sentence.append(word)\n",
    "            if word not in vocabulary.keys(): \n",
    "                vocabulary[word] = 1\n",
    "            else:\n",
    "                vocabulary[word] += 1\n",
    "                \n",
    "    final_sentence += ['</s>']\n",
    "    corpus += [\" \".join(final_sentence)] #adding processed sentence to the corpus\n",
    "\n",
    "#Now Processed Corpus is ready    \n",
    "number_of_sentence = len(corpus)\n",
    "# Vocabulary list doesn't contain the START and END word used in Pre-Processing\n",
    "vocabulary['<s>'] = 2*number_of_sentence\n",
    "vocabulary['</s>'] = 2*number_of_sentence\n",
    "# Now we have Processed sentence corpus and vocabulary list. \n",
    "#Processed Corpus DATA ANALYSIS  \n",
    "tokens = sum(vocabulary.values())\n",
    "types = len(vocabulary)\n",
    "print ('Number of Sentences = ' + str(number_of_sentence))\n",
    "print ('Number of Types = ' + str(types))\n",
    "print ('Number of Tokens = ' + str(tokens))\n",
    "print (\"TTR= \" + str(types/tokens))\n",
    "# train-test split in 80:20 ratio\n",
    "train_data, test_data = train_test_split(corpus, test_size=0.20, random_state=64)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> project gutenberg the complete works of jane austen by jane austen this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever </s>',\n",
       " '<s> you may copy it give it away or it under the terms of the project gutenberg license included with this ebook or online at title the complete project gutenberg works of jane austen author jane austen editor david widger release date january 25 2010 ebook 31100 language english character set encoding ascii start of this project gutenberg ebook the works of jane austen produced by many project gutenberg volunteers </s>',\n",
       " '<s> the works of jane austen edited by david widger project gutenberg editions dedication this jane austen collection is dedicated to alice goodson hart woodby note the accompanying html file has active links to all the volumes and chapters in this set </s>',\n",
       " '<s> contents persuasion northanger abbey mansfield park emma lady susan love and freindship and other early works pride and prejudice sense and sensibility persuasion by jane austen 1818 chapter sir walter elliot of kellynch hall in somersetshire was a man who for his own amusement never took up any book but the baronetage there he found occupation for an idle hour and consolation in a distressed one there his faculties were roused into admiration and respect by contemplating the limited remnant of the earliest patents there any unwelcome sensations arising from domestic affairs changed naturally into pity and contempt as he turned over the almost endless creations of the last century and there if every other leaf were powerless he could read his own history with an interest which never failed </s>',\n",
       " '<s> this was the page at which the favourite volume always opened elliot of kellynch hall </s>',\n",
       " '<s> walter elliot born march 1760 married july 15 1784 elizabeth daughter of james stevenson esq </s>',\n",
       " '<s> of south park in the county of gloucester by which lady who died 1800 he has issue elizabeth born june 1785 anne born august 1787 a son november 1789 mary born november 20 1791 </s>',\n",
       " '<s> precisely such had the paragraph originally stood from the hands but sir walter had improved it by adding for the information of himself and his family these words after the date of mary birth married december 16 1810 charles son and heir of charles musgrove esq </s>',\n",
       " '<s> of uppercross in the county of somerset and by inserting most accurately the day of the month on which he had lost his wife </s>',\n",
       " '<s> then followed the history and rise of the ancient and respectable family in the usual terms how it had been first settled in cheshire how mentioned in dugdale serving the office of high sheriff representing a borough in three successive parliaments exertions of loyalty and dignity of baronet in the first year of charles ii with all the marys and elizabeths they had married forming altogether two handsome duodecimo pages and concluding with the arms and motto principal seat kellynch hall in the county of somerset and sir walter handwriting again in this finale heir presumptive william walter elliot great grandson of the second sir walter </s>',\n",
       " '<s> vanity was the beginning and the end of sir walter elliot character vanity of person and of situation </s>',\n",
       " '<s> he had been remarkably handsome in his youth and at was still a very fine man </s>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1a MLE for N-Grams : N=  1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigrams\n",
    "MLE_u = {}           \n",
    "for i in vocabulary.keys():\n",
    "    MLE_u[i] = (float(vocabulary[i]))/tokens     \n",
    "\n",
    "\n",
    "# Bigrams\n",
    "\n",
    "MLE_b = {}# initilaizing empty biagram MLE\n",
    "bigram_list = {} # initilaizing empty biagram list\n",
    "# from each sentence in corpus extracting all the biagrams\n",
    "for sentence in corpus:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(1,len(words_in_sentence)):\n",
    "        j = (words_in_sentence[i-1], words_in_sentence[i])#taking all consecutive combination\n",
    "        try:\n",
    "            bigram_list[j] += 1\n",
    "        except:\n",
    "            bigram_list[j] = 1\n",
    "#NOw we have list a histogram of biagram in the corpus             \n",
    "for i in bigram_list.keys():\n",
    "    MLE_b[i] = float(bigram_list[i])/vocabulary[i[0]]\n",
    "\n",
    "\n",
    "# Trigrams\n",
    "MLE_t = {}#trigram MLE \n",
    "trigram_list = {}# initializing the list\n",
    "\n",
    "for sentence in corpus:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(2,len(words_in_sentence)):\n",
    "        j = (words_in_sentence[i-2], words_in_sentence[i-1], words_in_sentence[i])\n",
    "        try:\n",
    "            trigram_list[j] += 1\n",
    "        except:\n",
    "            trigram_list[j] = 1\n",
    "\n",
    "#NOw we have list a histogram of trigram in the corpus             \n",
    "for i in trigram_list.keys():\n",
    "    MLE_t[i] = (trigram_list[i]*1.0)/bigram_list[(i[0], i[1])]\n",
    "\n",
    "# Quadrigrams\n",
    "MLE_q = {}\n",
    "quadgram_list = {}\n",
    "\n",
    "for sentence in corpus:\n",
    "    words_in_sentence = sentence.split()\n",
    "    for i in range(3,len(words_in_sentence)):\n",
    "        j = (words_in_sentence[i-3], words_in_sentence[i-2], words_in_sentence[i-1], words_in_sentence[i])\n",
    "        try:\n",
    "            quadgram_list[j] += 1\n",
    "        except:\n",
    "            quadgram_list[j] = 1\n",
    "\n",
    "#NOw we have list a histogram of QUADgram in the corpus             \n",
    "for i in quadgram_list.keys():\n",
    "    MLE_q[i] = (quadgram_list[i]*1.0)/trigram_list[(i[0], i[1], i[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM-MLE - First 10\n",
      " [('2010', 1.0933275314085666e-06), ('31100', 1.0933275314085666e-06), ('encoding', 1.0933275314085666e-06), ('edited', 1.0933275314085666e-06), ('dedication', 1.0933275314085666e-06), ('goodson', 1.0933275314085666e-06), ('woodby', 1.0933275314085666e-06), ('html', 1.0933275314085666e-06), ('patents', 1.0933275314085666e-06), ('1760', 1.0933275314085666e-06)] \n",
      "BIGRAM-MLE - First 10\n",
      " [(('<s>', 'precisely'), 1.4262893655864902e-05), (('<s>', 'herself'), 1.4262893655864902e-05), (('<s>', 'journeys'), 1.4262893655864902e-05), (('<s>', 'quit'), 1.4262893655864902e-05), (('<s>', 'kellynch'), 1.4262893655864902e-05), (('<s>', 'therefore'), 1.4262893655864902e-05), (('<s>', 'sailors'), 1.4262893655864902e-05), (('<s>', 'fellow'), 1.4262893655864902e-05), (('<s>', 'picture'), 1.4262893655864902e-05), (('<s>', 'penelope'), 1.4262893655864902e-05)] \n",
      "TRIGRAM-MLE - First 10\n",
      " [(('<s>', 'i', 'venture'), 0.0002638522427440633), (('<s>', 'i', 'meet'), 0.0002638522427440633), (('<s>', 'i', 'brought'), 0.0002638522427440633), (('<s>', 'i', 'lived'), 0.0002638522427440633), (('<s>', 'i', 'kept'), 0.0002638522427440633), (('<s>', 'i', 'regard'), 0.0002638522427440633), (('<s>', 'i', 'walked'), 0.0002638522427440633), (('<s>', 'i', 'got'), 0.0002638522427440633), (('<s>', 'i', 'recollect'), 0.0002638522427440633), (('<s>', 'i', 'little'), 0.0002638522427440633)] \n",
      "QUADGRAM-MLE - First 10\n",
      " [(('<s>', 'i', 'am', 'every'), 0.0016835016835016834), (('<s>', 'i', 'am', 'engaged'), 0.0016835016835016834), (('<s>', 'i', 'am', 'proud'), 0.0016835016835016834), (('<s>', 'i', 'am', 'fond'), 0.0016835016835016834), (('<s>', 'i', 'am', 'undeceived'), 0.0016835016835016834), (('<s>', 'i', 'am', 'puzzled'), 0.0016835016835016834), (('<s>', 'i', 'am', 'assured'), 0.0016835016835016834), (('<s>', 'i', 'am', 'inclined'), 0.0016835016835016834), (('<s>', 'i', 'am', 'one'), 0.0016835016835016834), (('<s>', 'i', 'am', 'well'), 0.0016835016835016834)]\n"
     ]
    }
   ],
   "source": [
    "print(\"UNIGRAM-MLE - First 10\\n\",sorted(MLE_u.items(), key=lambda x:x[1])[:10],\n",
    "\"\\nBIGRAM-MLE - First 10\\n\",sorted(MLE_b.items(), key=lambda x:x[1])[:10],\n",
    "\"\\nTRIGRAM-MLE - First 10\\n\",sorted(MLE_t.items(), key=lambda x:x[1])[:10],\n",
    "\"\\nQUADGRAM-MLE - First 10\\n\",sorted(MLE_q.items(), key=lambda x:x[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter value of N:2\n",
      "Number of possible n-grams: 214651801\n",
      "Number of present n-grams: 213027\n"
     ]
    }
   ],
   "source": [
    "#TASK 1a . HOW MANY N-GRAMS are POSSIBLE AND HOW MANY ACTUALLY EXISTS\n",
    "def NGRAMS(N):\n",
    "    possible = types**N\n",
    "    list_of_NGRAMS = []\n",
    "    for sentence in corpus:\n",
    "        list_of_NGRAMS += list(ngrams(sentence.split(), N))\n",
    "    set_of_NGRAMS = set(list_of_NGRAMS)\n",
    "    present = len(set_of_NGRAMS)\n",
    "    return possible, present\n",
    "\n",
    "ngram = input('Enter value of N:')\n",
    "ngram=int(ngram)\n",
    "possible, present = NGRAMS(ngram)\n",
    "print (\"Number of possible n-grams: \" + str(possible))\n",
    "print (\"Number of present n-grams: \" + str(present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 1b - Defining Generator(model_name) for sentence generation \n",
    "\n",
    "# Using Inbuilt Sampling Function from  the Multinomial \n",
    "# GENRATOR FUNCTION DEFINATION\n",
    "\n",
    "def Generator(model_name):\n",
    "    if model_name==\"UNIGRAM\":\n",
    "        sentence = ''\n",
    "        start = \"<s>\"\n",
    "        sentence = sentence + start\n",
    "        probability = list(MLE_u.values())\n",
    "        wordlist = list(MLE_u.keys())\n",
    "        while(True):\n",
    "            arr = np.random.multinomial(1,probability)\n",
    "            word = wordlist[int(np.where(arr == 1)[0])]\n",
    "            if (word==\"<s>\"):\n",
    "                continue\n",
    "            elif (word == \"</s>\"):\n",
    "                sentence=sentence + word\n",
    "                print (sentence)\n",
    "                return\n",
    "            else:\n",
    "                sentence=sentence+word+\" \"\n",
    "                \n",
    "    elif model_name==\"BIGRAM\":\n",
    "        sentence=\"<s>\"\n",
    "        start = \"<s>\"\n",
    "        while(True):\n",
    "            nextw=[]\n",
    "            p=[]\n",
    "            for i in MLE_b.keys():\n",
    "                if(i[0]==start):\n",
    "                    nextw.append(i[1])\n",
    "            arr = (np.random.multinomial(1.000000000000000000000,p))\n",
    "            w = nextw[int(np.where(arr == 1)[0])]\n",
    "            if w == \"</s>\":\n",
    "                sentence=sentence+w\n",
    "                print (sentence)\n",
    "                return\n",
    "            else:\n",
    "                sentence=sentence+w+\" \"\n",
    "                start=w\n",
    "                \n",
    "    elif model_name==\"TRIGRAM\":\n",
    "        sentence = \" <s> \"\n",
    "        start = \"<s>\"\n",
    "        nextw=[]\n",
    "        p=[]\n",
    "        for i in MLE_b.keys():\n",
    "            if(i[0]==start):\n",
    "                nextw.append(i[1])\n",
    "                p.append(MLE_b[i])\n",
    "            \n",
    "        arr = np.random.multinomial(1,p)\n",
    "        w = nextw[int(np.where(arr==1)[0])]\n",
    "        bigTuple = (start,w)\n",
    "        sentence=sentence+w+\" \"\n",
    "            \n",
    "        while(True):\n",
    "            nextwT=[]\n",
    "            pT=[]\n",
    "            for i in MLE_t.keys():\n",
    "                if(i[0]==bigTuple[0] and i[1] == bigTuple[1]):\n",
    "                    nextwT.append(i[2])\n",
    "                    pT.append(MLE_t[i])\n",
    "            \n",
    "            arrT = np.random.multinomial(1,pT)\n",
    "            wT = nextwT[int(np.where(arrT==1)[0])]\n",
    "            bigTuple = (bigTuple[1],wT)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if wT == \"</s>\":\n",
    "                sentence=sentence+wT\n",
    "                print (sentence)\n",
    "                return\n",
    "            else:\n",
    "                sentence=sentence+wT+\" \"\n",
    "                \n",
    "                \n",
    "    elif model_name == \"QUADGRAM\":\n",
    "        sentence = \"<s> \"\n",
    "        start = \"<s>\"\n",
    "        nextwB=[]\n",
    "        pB=[]\n",
    "        for i in MLE_b.keys():\n",
    "            if(i[0]==start):\n",
    "                nextwB.append(i[1])\n",
    "                pB.append(MLE_b[i])\n",
    "            \n",
    "        arrB = np.random.multinomial(1,pB)\n",
    "        wB = nextwB[int(np.where(arrB==1)[0])]\n",
    "        bigTuple = (start,wB)\n",
    "        sentence=sentence+wB+\" \"\n",
    "            \n",
    "        nextwT=[]\n",
    "        pT=[]\n",
    "        for i in MLE_t.keys():\n",
    "            if(i[0] == bigTuple[0] and i[1]== bigTuple[1]):\n",
    "                nextwT.append(i[2])\n",
    "                pT.append(MLE_t[i])\n",
    "        \n",
    "        arrT = np.random.multinomial(1,pT)\n",
    "        wT = nextwT[int(np.where(arrT==1)[0])]\n",
    "        trigTuple = (start,wB,wT)\n",
    "        sentence=sentence+wT+\" \"\n",
    "            \n",
    "        while(True):\n",
    "            nextwQ=[]\n",
    "            pQ=[]\n",
    "            for i in MLE_q.keys():\n",
    "                if(i[0]==trigTuple[0] and i[1] == trigTuple[1] and i[2]==trigTuple[2]):\n",
    "                    nextwQ.append(i[3])\n",
    "                    pQ.append(MLE_q[i])\n",
    "            \n",
    "            arrQ = np.random.multinomial(1,pQ)\n",
    "            wQ = nextwQ[int(np.where(arrQ==1)[0])]\n",
    "            trigTuple = (trigTuple[1],trigTuple[2],wQ)       \n",
    "\n",
    "\n",
    "        if wQ == \"</s>\":\n",
    "            sentence=sentence+wQ\n",
    "            print (sentence)\n",
    "            return\n",
    "        else:\n",
    "            sentence=sentence+wQ+\" \"  \n",
    "\n",
    "print (\"----------------------------UNIGRAM SENTENCES----------------------------------\")\n",
    "for z in range(5):\n",
    "    Generator(\"UNIGRAM\")  \n",
    "    \n",
    "print (\"----------------------------BIGRAM SENTENCES-------------------------------------\")\n",
    "for z in range(5):\n",
    "    Generator(\"BIGRAM\")\n",
    "\n",
    "print (\"-----------------------------TRIGRAM SENTENCES----------------------------------\")\n",
    "for z in range(5):\n",
    "    Generator(\"TRIGRAM\")\n",
    "\n",
    "print (\"------------------------------QUADGRAM SENTENCES--------------------------------\")\n",
    "for z in range(5):\n",
    "    Generator(\"QUADGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Sentences are\n",
      "-------------------------\n",
      "<s>daughter she my you that they but </s>\n",
      "<s>speak man to </s>\n",
      "<s>and was chilly and so stay still she unfeeling compassion said not emma mary but everything in </s>\n",
      "<s>conceit the had about which i of sleek ashamed he father unnecessary like a have late world it nor camden take before wonder can example sure quite judge here body that one you what it from said act was spirits it be the someone henry not </s>\n",
      "<s>fears bed there my darcy should more as but not the the </s>\n",
      "Bigrams Sentences are\n",
      "-------------------------\n",
      "<s>professor michael hart woodby note was spent scarcely anything about she could but alas i was she grew impatient to her marriage explained them too much the more of the day following monday trusting their being a letter did not come cried isabella is i did not long very thoroughly fagged fanny shocked but what was not belong to avoid </s>\n",
      "<s>professor michael hart and warmth which was now on the course both sides there yet it and elegant and alienate the same profusion of fine spirited rejection of knoll they might afford we have come out of most was talking at hartfield to preserve myself rather in the alliance for the vanity were brightened at church </s>\n",
      "<s>professor michael hart and a formidable an estate </s>\n",
      "<s>your kind as some amusement never do not the kindness though it were the frequent opportunity of his affection neither suckling who was so well my dear creature he were in the girls had to consider their share of a brilliant cheerfulness to frank churchill is all her one of bingley shall see her father would have them commenced and mamma but john thorpe had given her aunt if there was unmanageable that to speak unnecessarily long as he would be but when occasion </s>\n",
      "<s>there was in </s>\n",
      "Trigrams Sentences are\n",
      "-------------------------\n",
      " <s> i am not very unlikely to occur with a child being in town and has fitted up for causes unknown and unthought of before </s>\n",
      " <s> professor michael hart is the case you see and hear more </s>\n",
      " <s> professor michael hart the owner in the world and if a smart young man indeed </s>\n",
      " <s> professor michael hart is the least resemblance to ease and good you would be no reason in the world he will be made comfortable and one still more and he told me that my real interest say all manner of bidding her adieu wishing her joy on her was indeed a mixture would be rather attentive than otherwise but that of any under a delusion totally ignorant of my actions </s>\n",
      " <s> it is usually correct </s>\n",
      "Quadgrams Sentences are\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-3c63a95054fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[0mgeneratorSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"quadgram\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-132-3c63a95054fe>\u001b[0m in \u001b[0;36mgeneratorSent\u001b[1;34m(modelName)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0marrQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mwQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnextwQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrQ\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mtrigTuple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrigTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrigTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "def sample_multinomial(w):\n",
    "    p = softmax(w)\n",
    "    x = np.random.uniform(0,1)\n",
    "    for i,v in enumerate(np.cumsum(p)):\n",
    "        if x < v: return i\n",
    "    return len(p)-1 \n",
    "\n",
    "def generatorSent(modelName):\n",
    "    if modelName==\"unigram\":\n",
    "        s = ''\n",
    "        start = \"<s>\"\n",
    "        s = s + start\n",
    "        prob = list(unigramMLE.values())\n",
    "        wordlist = list(unigramMLE.keys())\n",
    "        while(True):\n",
    "            arr = np.random.multinomial(1,prob)\n",
    "            word = wordlist[int(np.where(arr == 1)[0])]\n",
    "            if (word==\"<s>\"):\n",
    "                continue\n",
    "            elif (word == \"</s>\"):\n",
    "                s=s + word\n",
    "                print (s)\n",
    "                return\n",
    "            else:\n",
    "                s=s+word+\" \"\n",
    "                \n",
    "    elif modelName==\"bigram\":\n",
    "        s=\"<s>\"\n",
    "        start = \"<s>\"\n",
    "        while(True):\n",
    "            nextw=[]\n",
    "            p=[]\n",
    "            for i in bigramMLE.keys():\n",
    "                if(i[0]==start):\n",
    "                    nextw.append(i[1])\n",
    "                    p.append(bigramMLE[i])     # If you get an error ValueError: sum(pvals[:-1]) > 1.0 then it is because precision of the numbers \n",
    "                                                # in Python are not exact please Change your system to 32-bit and run it again. Or run this part by part\n",
    "            arr = (np.random.multinomial(1.000000000000000000000,p))\n",
    "            w = nextw[int(np.where(arr == 1)[0])]\n",
    "            if w == \"</s>\":\n",
    "                s=s+w\n",
    "                print (s)\n",
    "                return\n",
    "            else:\n",
    "                s=s+w+\" \"\n",
    "                start=w\n",
    "                \n",
    "    elif modelName==\"trigram\":\n",
    "        s = \" <s> \"\n",
    "        start = \"<s>\"\n",
    "        nextw=[]\n",
    "        p=[]\n",
    "        for i in bigramMLE.keys():\n",
    "            if(i[0]==start):\n",
    "                nextw.append(i[1])\n",
    "                p.append(bigramMLE[i])\n",
    "            \n",
    "        arr = np.random.multinomial(1,p)\n",
    "        w = nextw[int(np.where(arr==1)[0])]\n",
    "        bigTuple = (start,w)\n",
    "        s=s+w+\" \"\n",
    "            \n",
    "        while(True):\n",
    "            nextwT=[]\n",
    "            pT=[]\n",
    "            for i in trigramMLE.keys():\n",
    "                if(i[0]==bigTuple[0] and i[1] == bigTuple[1]):\n",
    "                    nextwT.append(i[2])\n",
    "                    pT.append(trigramMLE[i])\n",
    "            \n",
    "            arrT = np.random.multinomial(1,pT)\n",
    "            wT = nextwT[int(np.where(arrT==1)[0])]\n",
    "            bigTuple = (bigTuple[1],wT)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if wT == \"</s>\":\n",
    "                s=s+wT\n",
    "                print (s)\n",
    "                return\n",
    "            else:\n",
    "                s=s+wT+\" \"\n",
    "                \n",
    "                \n",
    "    elif modelName == \"quadgram\":\n",
    "        s = \"<s> \"\n",
    "        start = \"<s>\"\n",
    "        nextwB=[]\n",
    "        pB=[]\n",
    "        for i in MLE_b.keys():\n",
    "            if(i[0]==start):\n",
    "                nextwB.append(i[1])\n",
    "                pB.append(MLE_b[i])\n",
    "            \n",
    "        arrB = np.random.multinomial(1,pB)\n",
    "        wB = nextwB[int(np.where(arrB==1)[0])]\n",
    "        bigTuple = (start,wB)\n",
    "        s=s+wB+\" \"\n",
    "            \n",
    "        nextwT=[]\n",
    "        pT=[]\n",
    "        for i in MLE_t.keys():\n",
    "            if(i[0] == bigTuple[0] and i[1]== bigTuple[1]):\n",
    "                nextwT.append(i[2])\n",
    "                pT.append(MLE_t[i])\n",
    "        \n",
    "        arrT = np.random.multinomial(1,pT)\n",
    "        wT = nextwT[int(np.where(arrT==1)[0])]\n",
    "        trigTuple = (start,wB,wT)\n",
    "        s=s+wT+\" \"\n",
    "            \n",
    "        while(True):\n",
    "            nextwQ=[]\n",
    "            pQ=[]\n",
    "            for i in MLE_q.keys():\n",
    "                if(i[0]==trigTuple[0] and i[1] == trigTuple[1] and i[2]==trigTuple[2]):\n",
    "                    nextwQ.append(i[3])\n",
    "                    pQ.append(MLE_q[i])\n",
    "            \n",
    "            arrQ = np.random.multinomial(1,pQ)\n",
    "            wQ = nextwQ[int(np.where(arrQ==1)[0])]\n",
    "            trigTuple = (trigTuple[1],trigTuple[2],wQ)       \n",
    "            \n",
    "            \n",
    "            if wQ == \"</s>\":\n",
    "                s=s+wQ\n",
    "                print (s)\n",
    "                return\n",
    "            else:\n",
    "                s=s+wQ+\" \"      \n",
    "\n",
    "print (\"Unigram Sentences are\")\n",
    "print (\"-------------------------\")\n",
    "for z in range(5):\n",
    "    generatorSent(\"unigram\")  \n",
    "    \n",
    "print\n",
    "print\n",
    "print (\"Bigrams Sentences are\")\n",
    "print (\"-------------------------\")\n",
    "for z in range(5):\n",
    "    generatorSent(\"bigram\")\n",
    "\n",
    "print\n",
    "print\n",
    "print (\"Trigrams Sentences are\")\n",
    "print (\"-------------------------\")\n",
    "for z in range(5):\n",
    "    generatorSent(\"trigram\")\n",
    "\n",
    "print\n",
    "print\n",
    "print (\"Quadgrams Sentences are\")\n",
    "print (\"-------------------------\")\n",
    "for z in range(5):\n",
    "    generatorSent(\"quadgram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classical Approach and this code is for getting N-grams \n",
    "# we will pass the processed data to the generator and it will generate the n-grams where n=parameter\n",
    "def GENERATE_NGRAMS(text, N):\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Breaking sentence in the token, removing empty tokens- further <S> and </S> are tokens\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Concatenating the tokens into NGRAMS and returning using ZIP function\n",
    "    NGRAMS = zip(*[token[i:] for i in range(N)])\n",
    "    return [\" \".join(NGRAM) for NGRAM in NGRAMS]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classical Approach and this code make a GEnerator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is for evaluation of Classiscal Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this code for Neural approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this code evaluate the Neural Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
